{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMASY5gD9L0K"
   },
   "source": [
    "# **Lab1: Regression**\n",
    "In *lab 1*, you need to finish:\n",
    "\n",
    "1.  Basic Part: Implement the regression model to predict people's grip force from their weight.\n",
    "You can use either Matrix Inversion or Gradient Descent.\n",
    "\n",
    "\n",
    "> *   Step 1: Split Data\n",
    "> *   Step 2: Preprocess Data\n",
    "> *   Step 3: Implement Regression\n",
    "> *   Step 4: Make Prediction\n",
    "> *   Step 5: Train Model and Generate Result\n",
    "\n",
    "2.  Advanced Part: Implementing a regression model to predict grip force in a different way (for example, with more variables) than the basic part\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNpd_FfX_BXI"
   },
   "source": [
    "---\n",
    "# 1. Basic Part (50%)\n",
    "In the first part, you need to implement the regression to predict grip force\n",
    "\n",
    "Please save the prediction result in a CSV file and submit it to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egBMMLGV_X_x"
   },
   "source": [
    "### Import Packages\n",
    "\n",
    "> Note: You **cannot** import any other package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WhhUTua487C-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iuXHvhLALwz"
   },
   "source": [
    "### Global attributes\n",
    "Define the global attributes\\\n",
    "You can also add your own global attributes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wXZVhdp8-flF"
   },
   "outputs": [],
   "source": [
    "training_dataroot = 'lab1_basic_training.csv' # Training data file file named as 'lab1_basic_training.csv'\n",
    "testing_dataroot = 'lab1_basic_testing.csv'   # Testing data file named as 'lab1_basic_testing.csv'\n",
    "output_dataroot = 'lab1_basic.csv' # Output file will be named as 'lab1_basic.csv'\n",
    "\n",
    "training_datalist =  [] # Training datalist, saved as numpy array\n",
    "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
    "\n",
    "output_datalist =  [] # Your prediction, should be a list with 100 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyTqIRxQAtWj"
   },
   "source": [
    "### Load the Input File\n",
    "First, load the basic input file **lab1_basic_training.csv** and **lab1_basic_testing.csv**\n",
    "\n",
    "Input data would be stored in *training_datalist* and *testing_datalist*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KUzYjoq9AwRp"
   },
   "outputs": [],
   "source": [
    "# Read input csv to datalist\n",
    "with open(training_dataroot, newline='') as csvfile:\n",
    "  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
    "\n",
    "with open(testing_dataroot, newline='') as csvfile:\n",
    "  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFXG-axpAcom"
   },
   "source": [
    "### Implement the Regression Model\n",
    "\n",
    "> Note: It is recommended to use the functions we defined, you can also define your own functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bqYH_MvBv4v"
   },
   "source": [
    "#### Step 1: Split Data\n",
    "Split data in *training_datalist* into training dataset and validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6K2QUnt-A-1r"
   },
   "outputs": [],
   "source": [
    "def SplitData(data, split_ratio):\n",
    "    \"\"\"\n",
    "    Splits the given dataset into training and validation sets based on the specified split ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to be split. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "    - split_ratio (float): The ratio of the data to be used for training. For example, a value of 0.8 means 80% of the data will be used for training and the remaining 20% for validation.\n",
    "\n",
    "    Returns:\n",
    "    - training_data (numpy.ndarray): The portion of the dataset used for training.\n",
    "    - validation_data (numpy.ndarray): The portion of the dataset used for validation.\n",
    "\n",
    "    \"\"\"\n",
    "    mid = int(data.shape[0] * split_ratio)\n",
    "    training_data = data[:mid]\n",
    "    validation_data = data[mid:]\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return training_data, validation_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-miSLyewCeME"
   },
   "source": [
    "#### Step 2: Preprocess Data\n",
    "Handle unreasonable data and missing data\n",
    "\n",
    "> Hint 1: Outliers and missing data can be addressed by either removing them or replacing them using statistical methods (e.g., the mean of all data).\n",
    "\n",
    "> Hint 2: Missing data are represented as `np.nan`, so functions like `np.isnan()` can be used to detect them.\n",
    "\n",
    "> Hint 3: Methods such as the Interquartile Range (IQR) can help detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jR4TYnwwCrci"
   },
   "outputs": [],
   "source": [
    "def PreprocessData(data):\n",
    "    \"\"\"\n",
    "    Preprocess the given dataset and return the result.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to preprocess. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "\n",
    "    Returns:\n",
    "    - preprocessedData (numpy.ndarray): Preprocessed data.\n",
    "    \"\"\"\n",
    "    data = data[~np.isnan(data[:]).any(axis=1)]\n",
    "    q1 = np.percentile(data[:,0], 25)\n",
    "    q3 = np.percentile(data[:,0], 75)\n",
    "    \n",
    "    iqr = q3-q1\n",
    "    lowerbound = q1 - (1.5 * iqr)\n",
    "    upperbound = q3 + (1.5 * iqr)\n",
    "    \n",
    "    detect = (data[:, 0] >= lowerbound) & (data[:,0] <= upperbound)\n",
    "    preprocessedData = data[detect]\n",
    "\n",
    "    # TODO\n",
    "\n",
    "\n",
    "    return preprocessedData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csS9lL8DCzZO"
   },
   "source": [
    "### Step 3: Implement Regression\n",
    "You have to use Gradient Descent to finish this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n8ftprTRC0Na"
   },
   "outputs": [],
   "source": [
    "def Regression(dataset):\n",
    "    \"\"\"\n",
    "    Performs regression on the given dataset and return the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n",
    "\n",
    "    Returns:\n",
    "    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x + w[2] * x^2 + ...\n",
    "    \"\"\"\n",
    "    \n",
    "    X = dataset[:, :1]\n",
    "    y = dataset[:, 1]\n",
    "    # TODO: Decide on the degree of the polynomial\n",
    "    degree = 2  # For example, quadratic regression\n",
    "\n",
    "    # Add polynomial features to X\n",
    "    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, X ** d))  # Add x^d terms to feature matrix\n",
    "\n",
    "    # Initialize coefficients (weights) to zero\n",
    "    num_dimensions = X_poly.shape[1]  # Number of features (including intercept and polynomial terms)\n",
    "    w = np.zeros(num_dimensions)\n",
    "\n",
    "    # TODO: Set hyperparameters\n",
    "    num_iteration = 50000\n",
    "    learning_rate = 0.000000041\n",
    "\n",
    "\n",
    "    # Gradient Descent\n",
    "    m = len(y)  # Number of data points\n",
    "\n",
    "    for iteration in range(num_iteration):\n",
    "        # TODO: Prediction using current weights and compute error\n",
    "        y_predict = np.dot(X_poly, w)\n",
    "        #print(y_predict)\n",
    "        err = y_predict - y\n",
    "        #print(err)\n",
    "\n",
    "        # TODO: Compute gradient\n",
    "        gradient = 2/m * np.dot(X_poly.T, err)\n",
    "        #print(gradient)\n",
    "\n",
    "        # TODO: Update the weights\n",
    "        w -= (learning_rate * gradient)\n",
    "        #print(w)\n",
    "        \n",
    "        # TODO: Optionally, print the cost every 100 iterations\n",
    "        if iteration % 100 == 0:\n",
    "            cost = np.sum(err**2) / m\n",
    "            print(f\"Iteration {iteration}, Cost: {cost}\")\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inqQ4lh8DFY6"
   },
   "source": [
    "### Step 4: Make Prediction\n",
    "Make prediction of testing dataset and store the value in *output_datalist*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WwGE2qjgDLwt"
   },
   "outputs": [],
   "source": [
    "\n",
    "def MakePrediction(w, test_dataset):\n",
    "    \"\"\"\n",
    "    Predicts the output for a given test dataset using a regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n",
    "                               a coefficient for the respective power of the independent variable.\n",
    "    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n",
    "                                          for which predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    # TODO\n",
    "    degree = 2\n",
    "    X_poly = np.ones((test_dataset[:,:1].shape[0], 1))\n",
    "    for d in range(1, degree+1):\n",
    "        X_poly = np.hstack((X_poly, test_dataset[:, :1]**d))\n",
    "    print(X_poly)\n",
    "    print(w)\n",
    "    prediction = np.dot(X_poly, w)\n",
    "    print(prediction)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-q4qKXbDDmG9"
   },
   "source": [
    "### Step 5: Train Model and Generate Result\n",
    "\n",
    "Use the above functions to train your model on training dataset, and predict the answer of testing dataset.\n",
    "\n",
    "Save your predicted values in `output_datalist`\n",
    "\n",
    "> Notice: **Remember to inclue the coefficients of your model in the report**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "coo82WvZDpMq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Cost: 3164.3499778833498\n",
      "Iteration 100, Cost: 740.6930983177439\n",
      "Iteration 200, Cost: 719.8664926448713\n",
      "Iteration 300, Cost: 719.5503181981751\n",
      "Iteration 400, Cost: 719.4094932709212\n",
      "Iteration 500, Cost: 719.2704404785906\n",
      "Iteration 600, Cost: 719.1316776448226\n",
      "Iteration 700, Cost: 718.9931915700789\n",
      "Iteration 800, Cost: 718.8549815948521\n",
      "Iteration 900, Cost: 718.7170471677645\n",
      "Iteration 1000, Cost: 718.5793877394499\n",
      "Iteration 1100, Cost: 718.4420027616447\n",
      "Iteration 1200, Cost: 718.3048916871788\n",
      "Iteration 1300, Cost: 718.168053969973\n",
      "Iteration 1400, Cost: 718.0314890650365\n",
      "Iteration 1500, Cost: 717.8951964284649\n",
      "Iteration 1600, Cost: 717.7591755174392\n",
      "Iteration 1700, Cost: 717.6234257902213\n",
      "Iteration 1800, Cost: 717.487946706154\n",
      "Iteration 1900, Cost: 717.3527377256576\n",
      "Iteration 2000, Cost: 717.2177983102285\n",
      "Iteration 2100, Cost: 717.0831279224363\n",
      "Iteration 2200, Cost: 716.9487260259227\n",
      "Iteration 2300, Cost: 716.8145920853982\n",
      "Iteration 2400, Cost: 716.6807255666404\n",
      "Iteration 2500, Cost: 716.5471259364923\n",
      "Iteration 2600, Cost: 716.4137926628598\n",
      "Iteration 2700, Cost: 716.2807252147097\n",
      "Iteration 2800, Cost: 716.1479230620673\n",
      "Iteration 2900, Cost: 716.0153856760147\n",
      "Iteration 3000, Cost: 715.8831125286885\n",
      "Iteration 3100, Cost: 715.7511030932774\n",
      "Iteration 3200, Cost: 715.6193568440208\n",
      "Iteration 3300, Cost: 715.487873256206\n",
      "Iteration 3400, Cost: 715.3566518061666\n",
      "Iteration 3500, Cost: 715.2256919712803\n",
      "Iteration 3600, Cost: 715.0949932299663\n",
      "Iteration 3700, Cost: 714.9645550616846\n",
      "Iteration 3800, Cost: 714.8343769469318\n",
      "Iteration 3900, Cost: 714.7044583672409\n",
      "Iteration 4000, Cost: 714.5747988051787\n",
      "Iteration 4100, Cost: 714.4453977443435\n",
      "Iteration 4200, Cost: 714.3162546693626\n",
      "Iteration 4300, Cost: 714.1873690658917\n",
      "Iteration 4400, Cost: 714.0587404206112\n",
      "Iteration 4500, Cost: 713.9303682212253\n",
      "Iteration 4600, Cost: 713.8022519564594\n",
      "Iteration 4700, Cost: 713.6743911160581\n",
      "Iteration 4800, Cost: 713.5467851907838\n",
      "Iteration 4900, Cost: 713.4194336724136\n",
      "Iteration 5000, Cost: 713.292336053738\n",
      "Iteration 5100, Cost: 713.165491828559\n",
      "Iteration 5200, Cost: 713.0389004916875\n",
      "Iteration 5300, Cost: 712.9125615389416\n",
      "Iteration 5400, Cost: 712.786474467145\n",
      "Iteration 5500, Cost: 712.6606387741236\n",
      "Iteration 5600, Cost: 712.5350539587059\n",
      "Iteration 5700, Cost: 712.4097195207182\n",
      "Iteration 5800, Cost: 712.2846349609848\n",
      "Iteration 5900, Cost: 712.159799781325\n",
      "Iteration 6000, Cost: 712.0352134845514\n",
      "Iteration 6100, Cost: 711.9108755744678\n",
      "Iteration 6200, Cost: 711.786785555867\n",
      "Iteration 6300, Cost: 711.6629429345297\n",
      "Iteration 6400, Cost: 711.539347217221\n",
      "Iteration 6500, Cost: 711.4159979116904\n",
      "Iteration 6600, Cost: 711.2928945266682\n",
      "Iteration 6700, Cost: 711.1700365718639\n",
      "Iteration 6800, Cost: 711.0474235579651\n",
      "Iteration 6900, Cost: 710.9250549966348\n",
      "Iteration 7000, Cost: 710.8029304005094\n",
      "Iteration 7100, Cost: 710.6810492831968\n",
      "Iteration 7200, Cost: 710.5594111592746\n",
      "Iteration 7300, Cost: 710.4380155442891\n",
      "Iteration 7400, Cost: 710.3168619547515\n",
      "Iteration 7500, Cost: 710.1959499081368\n",
      "Iteration 7600, Cost: 710.075278922883\n",
      "Iteration 7700, Cost: 709.9548485183872\n",
      "Iteration 7800, Cost: 709.8346582150052\n",
      "Iteration 7900, Cost: 709.714707534049\n",
      "Iteration 8000, Cost: 709.5949959977846\n",
      "Iteration 8100, Cost: 709.4755231294313\n",
      "Iteration 8200, Cost: 709.3562884531576\n",
      "Iteration 8300, Cost: 709.237291494082\n",
      "Iteration 8400, Cost: 709.1185317782694\n",
      "Iteration 8500, Cost: 709.0000088327289\n",
      "Iteration 8600, Cost: 708.8817221854133\n",
      "Iteration 8700, Cost: 708.7636713652165\n",
      "Iteration 8800, Cost: 708.6458559019713\n",
      "Iteration 8900, Cost: 708.528275326448\n",
      "Iteration 9000, Cost: 708.4109291703525\n",
      "Iteration 9100, Cost: 708.2938169663245\n",
      "Iteration 9200, Cost: 708.1769382479347\n",
      "Iteration 9300, Cost: 708.0602925496846\n",
      "Iteration 9400, Cost: 707.9438794070035\n",
      "Iteration 9500, Cost: 707.8276983562466\n",
      "Iteration 9600, Cost: 707.7117489346941\n",
      "Iteration 9700, Cost: 707.5960306805478\n",
      "Iteration 9800, Cost: 707.480543132931\n",
      "Iteration 9900, Cost: 707.3652858318856\n",
      "Iteration 10000, Cost: 707.2502583183701\n",
      "Iteration 10100, Cost: 707.1354601342591\n",
      "Iteration 10200, Cost: 707.0208908223397\n",
      "Iteration 10300, Cost: 706.9065499263107\n",
      "Iteration 10400, Cost: 706.7924369907813\n",
      "Iteration 10500, Cost: 706.6785515612677\n",
      "Iteration 10600, Cost: 706.5648931841926\n",
      "Iteration 10700, Cost: 706.451461406883\n",
      "Iteration 10800, Cost: 706.3382557775685\n",
      "Iteration 10900, Cost: 706.2252758453793\n",
      "Iteration 11000, Cost: 706.1125211603444\n",
      "Iteration 11100, Cost: 705.9999912733899\n",
      "Iteration 11200, Cost: 705.8876857363374\n",
      "Iteration 11300, Cost: 705.775604101902\n",
      "Iteration 11400, Cost: 705.6637459236902\n",
      "Iteration 11500, Cost: 705.5521107561989\n",
      "Iteration 11600, Cost: 705.4406981548132\n",
      "Iteration 11700, Cost: 705.3295076758039\n",
      "Iteration 11800, Cost: 705.2185388763277\n",
      "Iteration 11900, Cost: 705.1077913144231\n",
      "Iteration 12000, Cost: 704.9972645490104\n",
      "Iteration 12100, Cost: 704.886958139889\n",
      "Iteration 12200, Cost: 704.7768716477357\n",
      "Iteration 12300, Cost: 704.6670046341037\n",
      "Iteration 12400, Cost: 704.5573566614198\n",
      "Iteration 12500, Cost: 704.4479272929838\n",
      "Iteration 12600, Cost: 704.3387160929655\n",
      "Iteration 12700, Cost: 704.2297226264042\n",
      "Iteration 12800, Cost: 704.1209464592057\n",
      "Iteration 12900, Cost: 704.0123871581419\n",
      "Iteration 13000, Cost: 703.9040442908481\n",
      "Iteration 13100, Cost: 703.7959174258216\n",
      "Iteration 13200, Cost: 703.6880061324202\n",
      "Iteration 13300, Cost: 703.58030998086\n",
      "Iteration 13400, Cost: 703.4728285422141\n",
      "Iteration 13500, Cost: 703.3655613884106\n",
      "Iteration 13600, Cost: 703.2585080922315\n",
      "Iteration 13700, Cost: 703.1516682273099\n",
      "Iteration 13800, Cost: 703.0450413681292\n",
      "Iteration 13900, Cost: 702.9386270900214\n",
      "Iteration 14000, Cost: 702.8324249691649\n",
      "Iteration 14100, Cost: 702.7264345825832\n",
      "Iteration 14200, Cost: 702.6206555081429\n",
      "Iteration 14300, Cost: 702.5150873245525\n",
      "Iteration 14400, Cost: 702.4097296113599\n",
      "Iteration 14500, Cost: 702.304581948952\n",
      "Iteration 14600, Cost: 702.1996439185516\n",
      "Iteration 14700, Cost: 702.094915102217\n",
      "Iteration 14800, Cost: 701.990395082839\n",
      "Iteration 14900, Cost: 701.8860834441407\n",
      "Iteration 15000, Cost: 701.7819797706749\n",
      "Iteration 15100, Cost: 701.6780836478225\n",
      "Iteration 15200, Cost: 701.5743946617912\n",
      "Iteration 15300, Cost: 701.4709123996136\n",
      "Iteration 15400, Cost: 701.3676364491457\n",
      "Iteration 15500, Cost: 701.2645663990652\n",
      "Iteration 15600, Cost: 701.1617018388698\n",
      "Iteration 15700, Cost: 701.0590423588753\n",
      "Iteration 15800, Cost: 700.9565875502151\n",
      "Iteration 15900, Cost: 700.8543370048371\n",
      "Iteration 16000, Cost: 700.7522903155027\n",
      "Iteration 16100, Cost: 700.6504470757853\n",
      "Iteration 16200, Cost: 700.5488068800693\n",
      "Iteration 16300, Cost: 700.4473693235465\n",
      "Iteration 16400, Cost: 700.3461340022166\n",
      "Iteration 16500, Cost: 700.2451005128847\n",
      "Iteration 16600, Cost: 700.1442684531597\n",
      "Iteration 16700, Cost: 700.0436374214523\n",
      "Iteration 16800, Cost: 699.9432070169747\n",
      "Iteration 16900, Cost: 699.8429768397374\n",
      "Iteration 17000, Cost: 699.7429464905485\n",
      "Iteration 17100, Cost: 699.6431155710123\n",
      "Iteration 17200, Cost: 699.543483683527\n",
      "Iteration 17300, Cost: 699.4440504312839\n",
      "Iteration 17400, Cost: 699.344815418265\n",
      "Iteration 17500, Cost: 699.2457782492419\n",
      "Iteration 17600, Cost: 699.1469385297745\n",
      "Iteration 17700, Cost: 699.0482958662088\n",
      "Iteration 17800, Cost: 698.9498498656754\n",
      "Iteration 17900, Cost: 698.851600136089\n",
      "Iteration 18000, Cost: 698.7535462861451\n",
      "Iteration 18100, Cost: 698.6556879253199\n",
      "Iteration 18200, Cost: 698.5580246638677\n",
      "Iteration 18300, Cost: 698.4605561128204\n",
      "Iteration 18400, Cost: 698.3632818839849\n",
      "Iteration 18500, Cost: 698.2662015899423\n",
      "Iteration 18600, Cost: 698.1693148440461\n",
      "Iteration 18700, Cost: 698.0726212604205\n",
      "Iteration 18800, Cost: 697.9761204539592\n",
      "Iteration 18900, Cost: 697.8798120403233\n",
      "Iteration 19000, Cost: 697.7836956359407\n",
      "Iteration 19100, Cost: 697.687770858004\n",
      "Iteration 19200, Cost: 697.5920373244682\n",
      "Iteration 19300, Cost: 697.4964946540509\n",
      "Iteration 19400, Cost: 697.4011424662297\n",
      "Iteration 19500, Cost: 697.3059803812401\n",
      "Iteration 19600, Cost: 697.2110080200756\n",
      "Iteration 19700, Cost: 697.1162250044854\n",
      "Iteration 19800, Cost: 697.0216309569716\n",
      "Iteration 19900, Cost: 696.9272255007903\n",
      "Iteration 20000, Cost: 696.833008259948\n",
      "Iteration 20100, Cost: 696.7389788592011\n",
      "Iteration 20200, Cost: 696.6451369240539\n",
      "Iteration 20300, Cost: 696.5514820807572\n",
      "Iteration 20400, Cost: 696.4580139563075\n",
      "Iteration 20500, Cost: 696.3647321784446\n",
      "Iteration 20600, Cost: 696.2716363756502\n",
      "Iteration 20700, Cost: 696.1787261771473\n",
      "Iteration 20800, Cost: 696.0860012128974\n",
      "Iteration 20900, Cost: 695.9934611136005\n",
      "Iteration 21000, Cost: 695.9011055106926\n",
      "Iteration 21100, Cost: 695.808934036344\n",
      "Iteration 21200, Cost: 695.7169463234591\n",
      "Iteration 21300, Cost: 695.6251420056736\n",
      "Iteration 21400, Cost: 695.5335207173541\n",
      "Iteration 21500, Cost: 695.4420820935956\n",
      "Iteration 21600, Cost: 695.3508257702214\n",
      "Iteration 21700, Cost: 695.2597513837798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21800, Cost: 695.1688585715445\n",
      "Iteration 21900, Cost: 695.0781469715125\n",
      "Iteration 22000, Cost: 694.9876162224018\n",
      "Iteration 22100, Cost: 694.897265963651\n",
      "Iteration 22200, Cost: 694.807095835418\n",
      "Iteration 22300, Cost: 694.7171054785774\n",
      "Iteration 22400, Cost: 694.6272945347202\n",
      "Iteration 22500, Cost: 694.537662646152\n",
      "Iteration 22600, Cost: 694.448209455891\n",
      "Iteration 22700, Cost: 694.358934607668\n",
      "Iteration 22800, Cost: 694.2698377459234\n",
      "Iteration 22900, Cost: 694.1809185158065\n",
      "Iteration 23000, Cost: 694.0921765631747\n",
      "Iteration 23100, Cost: 694.0036115345907\n",
      "Iteration 23200, Cost: 693.9152230773221\n",
      "Iteration 23300, Cost: 693.82701083934\n",
      "Iteration 23400, Cost: 693.7389744693168\n",
      "Iteration 23500, Cost: 693.651113616626\n",
      "Iteration 23600, Cost: 693.5634279313396\n",
      "Iteration 23700, Cost: 693.4759170642275\n",
      "Iteration 23800, Cost: 693.3885806667558\n",
      "Iteration 23900, Cost: 693.3014183910857\n",
      "Iteration 24000, Cost: 693.2144298900714\n",
      "Iteration 24100, Cost: 693.1276148172593\n",
      "Iteration 24200, Cost: 693.0409728268871\n",
      "Iteration 24300, Cost: 692.9545035738814\n",
      "Iteration 24400, Cost: 692.8682067138566\n",
      "Iteration 24500, Cost: 692.7820819031139\n",
      "Iteration 24600, Cost: 692.6961287986398\n",
      "Iteration 24700, Cost: 692.6103470581047\n",
      "Iteration 24800, Cost: 692.5247363398614\n",
      "Iteration 24900, Cost: 692.4392963029436\n",
      "Iteration 25000, Cost: 692.3540266070653\n",
      "Iteration 25100, Cost: 692.2689269126184\n",
      "Iteration 25200, Cost: 692.1839968806725\n",
      "Iteration 25300, Cost: 692.0992361729719\n",
      "Iteration 25400, Cost: 692.0146444519366\n",
      "Iteration 25500, Cost: 691.9302213806585\n",
      "Iteration 25600, Cost: 691.8459666229018\n",
      "Iteration 25700, Cost: 691.761879843101\n",
      "Iteration 25800, Cost: 691.6779607063595\n",
      "Iteration 25900, Cost: 691.5942088784482\n",
      "Iteration 26000, Cost: 691.5106240258048\n",
      "Iteration 26100, Cost: 691.4272058155316\n",
      "Iteration 26200, Cost: 691.3439539153945\n",
      "Iteration 26300, Cost: 691.2608679938227\n",
      "Iteration 26400, Cost: 691.1779477199052\n",
      "Iteration 26500, Cost: 691.0951927633914\n",
      "Iteration 26600, Cost: 691.0126027946895\n",
      "Iteration 26700, Cost: 690.9301774848639\n",
      "Iteration 26800, Cost: 690.8479165056353\n",
      "Iteration 26900, Cost: 690.7658195293789\n",
      "Iteration 27000, Cost: 690.6838862291229\n",
      "Iteration 27100, Cost: 690.6021162785478\n",
      "Iteration 27200, Cost: 690.520509351984\n",
      "Iteration 27300, Cost: 690.4390651244117\n",
      "Iteration 27400, Cost: 690.3577832714591\n",
      "Iteration 27500, Cost: 690.2766634694007\n",
      "Iteration 27600, Cost: 690.1957053951568\n",
      "Iteration 27700, Cost: 690.1149087262916\n",
      "Iteration 27800, Cost: 690.0342731410124\n",
      "Iteration 27900, Cost: 689.9537983181676\n",
      "Iteration 28000, Cost: 689.8734839372464\n",
      "Iteration 28100, Cost: 689.7933296783766\n",
      "Iteration 28200, Cost: 689.7133352223241\n",
      "Iteration 28300, Cost: 689.6335002504908\n",
      "Iteration 28400, Cost: 689.553824444914\n",
      "Iteration 28500, Cost: 689.474307488265\n",
      "Iteration 28600, Cost: 689.3949490638477\n",
      "Iteration 28700, Cost: 689.3157488555976\n",
      "Iteration 28800, Cost: 689.2367065480797\n",
      "Iteration 28900, Cost: 689.1578218264889\n",
      "Iteration 29000, Cost: 689.0790943766465\n",
      "Iteration 29100, Cost: 689.0005238850013\n",
      "Iteration 29200, Cost: 688.9221100386263\n",
      "Iteration 29300, Cost: 688.8438525252192\n",
      "Iteration 29400, Cost: 688.7657510330998\n",
      "Iteration 29500, Cost: 688.6878052512095\n",
      "Iteration 29600, Cost: 688.6100148691097\n",
      "Iteration 29700, Cost: 688.5323795769809\n",
      "Iteration 29800, Cost: 688.4548990656211\n",
      "Iteration 29900, Cost: 688.3775730264451\n",
      "Iteration 30000, Cost: 688.3004011514826\n",
      "Iteration 30100, Cost: 688.223383133377\n",
      "Iteration 30200, Cost: 688.1465186653853\n",
      "Iteration 30300, Cost: 688.0698074413754\n",
      "Iteration 30400, Cost: 687.993249155826\n",
      "Iteration 30500, Cost: 687.9168435038243\n",
      "Iteration 30600, Cost: 687.840590181066\n",
      "Iteration 30700, Cost: 687.7644888838533\n",
      "Iteration 30800, Cost: 687.6885393090936\n",
      "Iteration 30900, Cost: 687.6127411542988\n",
      "Iteration 31000, Cost: 687.5370941175844\n",
      "Iteration 31100, Cost: 687.4615978976666\n",
      "Iteration 31200, Cost: 687.3862521938631\n",
      "Iteration 31300, Cost: 687.311056706091\n",
      "Iteration 31400, Cost: 687.2360111348655\n",
      "Iteration 31500, Cost: 687.1611151812989\n",
      "Iteration 31600, Cost: 687.0863685470994\n",
      "Iteration 31700, Cost: 687.01177093457\n",
      "Iteration 31800, Cost: 686.937322046607\n",
      "Iteration 31900, Cost: 686.8630215866993\n",
      "Iteration 32000, Cost: 686.7888692589268\n",
      "Iteration 32100, Cost: 686.7148647679593\n",
      "Iteration 32200, Cost: 686.6410078190556\n",
      "Iteration 32300, Cost: 686.5672981180622\n",
      "Iteration 32400, Cost: 686.4937353714115\n",
      "Iteration 32500, Cost: 686.4203192861219\n",
      "Iteration 32600, Cost: 686.3470495697952\n",
      "Iteration 32700, Cost: 686.2739259306168\n",
      "Iteration 32800, Cost: 686.2009480773536\n",
      "Iteration 32900, Cost: 686.1281157193529\n",
      "Iteration 33000, Cost: 686.0554285665421\n",
      "Iteration 33100, Cost: 685.9828863294263\n",
      "Iteration 33200, Cost: 685.9104887190881\n",
      "Iteration 33300, Cost: 685.8382354471856\n",
      "Iteration 33400, Cost: 685.766126225953\n",
      "Iteration 33500, Cost: 685.6941607681966\n",
      "Iteration 33600, Cost: 685.6223387872964\n",
      "Iteration 33700, Cost: 685.5506599972036\n",
      "Iteration 33800, Cost: 685.4791241124393\n",
      "Iteration 33900, Cost: 685.4077308480944\n",
      "Iteration 34000, Cost: 685.3364799198271\n",
      "Iteration 34100, Cost: 685.2653710438633\n",
      "Iteration 34200, Cost: 685.1944039369938\n",
      "Iteration 34300, Cost: 685.1235783165747\n",
      "Iteration 34400, Cost: 685.052893900525\n",
      "Iteration 34500, Cost: 684.982350407327\n",
      "Iteration 34600, Cost: 684.9119475560235\n",
      "Iteration 34700, Cost: 684.8416850662176\n",
      "Iteration 34800, Cost: 684.7715626580713\n",
      "Iteration 34900, Cost: 684.7015800523047\n",
      "Iteration 35000, Cost: 684.631736970195\n",
      "Iteration 35100, Cost: 684.5620331335746\n",
      "Iteration 35200, Cost: 684.4924682648306\n",
      "Iteration 35300, Cost: 684.4230420869036\n",
      "Iteration 35400, Cost: 684.3537543232868\n",
      "Iteration 35500, Cost: 684.2846046980239\n",
      "Iteration 35600, Cost: 684.2155929357101\n",
      "Iteration 35700, Cost: 684.1467187614886\n",
      "Iteration 35800, Cost: 684.0779819010507\n",
      "Iteration 35900, Cost: 684.009382080635\n",
      "Iteration 36000, Cost: 683.9409190270256\n",
      "Iteration 36100, Cost: 683.8725924675514\n",
      "Iteration 36200, Cost: 683.8044021300849\n",
      "Iteration 36300, Cost: 683.736347743041\n",
      "Iteration 36400, Cost: 683.6684290353764\n",
      "Iteration 36500, Cost: 683.6006457365881\n",
      "Iteration 36600, Cost: 683.532997576712\n",
      "Iteration 36700, Cost: 683.4654842863225\n",
      "Iteration 36800, Cost: 683.3981055965315\n",
      "Iteration 36900, Cost: 683.3308612389862\n",
      "Iteration 37000, Cost: 683.2637509458697\n",
      "Iteration 37100, Cost: 683.1967744498982\n",
      "Iteration 37200, Cost: 683.1299314843213\n",
      "Iteration 37300, Cost: 683.0632217829203\n",
      "Iteration 37400, Cost: 682.9966450800074\n",
      "Iteration 37500, Cost: 682.9302011104242\n",
      "Iteration 37600, Cost: 682.8638896095412\n",
      "Iteration 37700, Cost: 682.7977103132562\n",
      "Iteration 37800, Cost: 682.7316629579939\n",
      "Iteration 37900, Cost: 682.6657472807044\n",
      "Iteration 38000, Cost: 682.599963018862\n",
      "Iteration 38100, Cost: 682.5343099104646\n",
      "Iteration 38200, Cost: 682.4687876940324\n",
      "Iteration 38300, Cost: 682.4033961086068\n",
      "Iteration 38400, Cost: 682.3381348937497\n",
      "Iteration 38500, Cost: 682.2730037895423\n",
      "Iteration 38600, Cost: 682.2080025365836\n",
      "Iteration 38700, Cost: 682.1431308759901\n",
      "Iteration 38800, Cost: 682.078388549394\n",
      "Iteration 38900, Cost: 682.0137752989435\n",
      "Iteration 39000, Cost: 681.9492908672999\n",
      "Iteration 39100, Cost: 681.884934997638\n",
      "Iteration 39200, Cost: 681.8207074336447\n",
      "Iteration 39300, Cost: 681.7566079195178\n",
      "Iteration 39400, Cost: 681.6926361999649\n",
      "Iteration 39500, Cost: 681.628792020203\n",
      "Iteration 39600, Cost: 681.5650751259568\n",
      "Iteration 39700, Cost: 681.5014852634577\n",
      "Iteration 39800, Cost: 681.4380221794437\n",
      "Iteration 39900, Cost: 681.374685621157\n",
      "Iteration 40000, Cost: 681.3114753363442\n",
      "Iteration 40100, Cost: 681.2483910732547\n",
      "Iteration 40200, Cost: 681.1854325806399\n",
      "Iteration 40300, Cost: 681.1225996077519\n",
      "Iteration 40400, Cost: 681.0598919043425\n",
      "Iteration 40500, Cost: 680.9973092206634\n",
      "Iteration 40600, Cost: 680.934851307463\n",
      "Iteration 40700, Cost: 680.872517915987\n",
      "Iteration 40800, Cost: 680.8103087979781\n",
      "Iteration 40900, Cost: 680.7482237056726\n",
      "Iteration 41000, Cost: 680.6862623918008\n",
      "Iteration 41100, Cost: 680.6244246095869\n",
      "Iteration 41200, Cost: 680.5627101127468\n",
      "Iteration 41300, Cost: 680.5011186554866\n",
      "Iteration 41400, Cost: 680.4396499925039\n",
      "Iteration 41500, Cost: 680.3783038789837\n",
      "Iteration 41600, Cost: 680.3170800706005\n",
      "Iteration 41700, Cost: 680.2559783235148\n",
      "Iteration 41800, Cost: 680.1949983943742\n",
      "Iteration 41900, Cost: 680.1341400403109\n",
      "Iteration 42000, Cost: 680.0734030189416\n",
      "Iteration 42100, Cost: 680.0127870883657\n",
      "Iteration 42200, Cost: 679.9522920071656\n",
      "Iteration 42300, Cost: 679.8919175344043\n",
      "Iteration 42400, Cost: 679.8316634296259\n",
      "Iteration 42500, Cost: 679.7715294528533\n",
      "Iteration 42600, Cost: 679.7115153645882\n",
      "Iteration 42700, Cost: 679.6516209258095\n",
      "Iteration 42800, Cost: 679.591845897973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42900, Cost: 679.5321900430093\n",
      "Iteration 43000, Cost: 679.4726531233249\n",
      "Iteration 43100, Cost: 679.4132349017989\n",
      "Iteration 43200, Cost: 679.3539351417836\n",
      "Iteration 43300, Cost: 679.294753607103\n",
      "Iteration 43400, Cost: 679.2356900620521\n",
      "Iteration 43500, Cost: 679.1767442713955\n",
      "Iteration 43600, Cost: 679.1179160003675\n",
      "Iteration 43700, Cost: 679.0592050146693\n",
      "Iteration 43800, Cost: 679.0006110804703\n",
      "Iteration 43900, Cost: 678.9421339644057\n",
      "Iteration 44000, Cost: 678.8837734335757\n",
      "Iteration 44100, Cost: 678.8255292555451\n",
      "Iteration 44200, Cost: 678.7674011983422\n",
      "Iteration 44300, Cost: 678.7093890304576\n",
      "Iteration 44400, Cost: 678.6514925208434\n",
      "Iteration 44500, Cost: 678.5937114389125\n",
      "Iteration 44600, Cost: 678.5360455545375\n",
      "Iteration 44700, Cost: 678.4784946380496\n",
      "Iteration 44800, Cost: 678.4210584602383\n",
      "Iteration 44900, Cost: 678.3637367923496\n",
      "Iteration 45000, Cost: 678.3065294060858\n",
      "Iteration 45100, Cost: 678.2494360736048\n",
      "Iteration 45200, Cost: 678.1924565675178\n",
      "Iteration 45300, Cost: 678.1355906608895\n",
      "Iteration 45400, Cost: 678.0788381272382\n",
      "Iteration 45500, Cost: 678.0221987405326\n",
      "Iteration 45600, Cost: 677.9656722751923\n",
      "Iteration 45700, Cost: 677.9092585060866\n",
      "Iteration 45800, Cost: 677.852957208534\n",
      "Iteration 45900, Cost: 677.7967681583004\n",
      "Iteration 46000, Cost: 677.7406911315992\n",
      "Iteration 46100, Cost: 677.6847259050896\n",
      "Iteration 46200, Cost: 677.6288722558766\n",
      "Iteration 46300, Cost: 677.5731299615088\n",
      "Iteration 46400, Cost: 677.5174987999791\n",
      "Iteration 46500, Cost: 677.4619785497225\n",
      "Iteration 46600, Cost: 677.4065689896161\n",
      "Iteration 46700, Cost: 677.3512698989774\n",
      "Iteration 46800, Cost: 677.2960810575642\n",
      "Iteration 46900, Cost: 677.2410022455734\n",
      "Iteration 47000, Cost: 677.1860332436398\n",
      "Iteration 47100, Cost: 677.1311738328362\n",
      "Iteration 47200, Cost: 677.076423794671\n",
      "Iteration 47300, Cost: 677.021782911089\n",
      "Iteration 47400, Cost: 676.9672509644693\n",
      "Iteration 47500, Cost: 676.912827737625\n",
      "Iteration 47600, Cost: 676.8585130138018\n",
      "Iteration 47700, Cost: 676.8043065766786\n",
      "Iteration 47800, Cost: 676.7502082103645\n",
      "Iteration 47900, Cost: 676.6962176993993\n",
      "Iteration 48000, Cost: 676.6423348287526\n",
      "Iteration 48100, Cost: 676.5885593838226\n",
      "Iteration 48200, Cost: 676.5348911504352\n",
      "Iteration 48300, Cost: 676.4813299148437\n",
      "Iteration 48400, Cost: 676.4278754637272\n",
      "Iteration 48500, Cost: 676.37452758419\n",
      "Iteration 48600, Cost: 676.321286063761\n",
      "Iteration 48700, Cost: 676.2681506903929\n",
      "Iteration 48800, Cost: 676.215121252461\n",
      "Iteration 48900, Cost: 676.1621975387621\n",
      "Iteration 49000, Cost: 676.1093793385149\n",
      "Iteration 49100, Cost: 676.0566664413578\n",
      "Iteration 49200, Cost: 676.0040586373486\n",
      "Iteration 49300, Cost: 675.9515557169639\n",
      "Iteration 49400, Cost: 675.8991574710976\n",
      "Iteration 49500, Cost: 675.8468636910611\n",
      "Iteration 49600, Cost: 675.7946741685813\n",
      "Iteration 49700, Cost: 675.7425886958008\n",
      "Iteration 49800, Cost: 675.690607065276\n",
      "Iteration 49900, Cost: 675.6387290699777\n",
      "[0.00843566 0.29855564 0.00598134]\n",
      "[[1.00000e+00 5.68000e+01 3.22624e+03]\n",
      " [1.00000e+00 7.64000e+01 5.83696e+03]\n",
      " [1.00000e+00 5.31000e+01 2.81961e+03]\n",
      " ...\n",
      " [1.00000e+00 6.53000e+01 4.26409e+03]\n",
      " [1.00000e+00 6.27000e+01 3.93129e+03]\n",
      " [1.00000e+00 5.97000e+01 3.56409e+03]]\n",
      "[0.00843566 0.29855564 0.00598134]\n",
      "[36.26364781 57.73095309 32.72679798 54.26499163 45.87671216 60.42741385\n",
      " 58.58263747 51.81796159 33.57203981 65.60713428 63.30834791 56.52442942\n",
      " 34.23614924 53.67758036 49.31098981 30.52019199 40.37414317 52.04873916\n",
      " 46.42285216 62.67667469 59.68627337 33.76118723 58.70478517 34.90612038\n",
      " 52.39580271 33.85594038 43.50922533 37.74416033 51.12849992 61.17286089\n",
      " 61.54719938 49.42368699 67.94467976 56.64454346 33.57203981 52.39580271\n",
      " 45.87671216 53.32656911 45.11714049 38.1435095  65.86494753 60.79959905\n",
      " 67.68303845 28.29491555 51.81796159 45.00910875 61.67221813 38.1435095\n",
      " 44.4707445  54.97383285 27.68523636 49.08595433 40.5797991  36.65581936\n",
      " 48.3020984  90.57648899 49.76249629 80.2137938  67.16119136 44.04220638\n",
      " 39.35304116 32.72679798 48.86139736 62.9289851  59.80949772 46.97198284\n",
      " 38.54477269 54.38283277 29.17605411 54.38283277 56.2845602  29.88957809\n",
      " 58.70478517 65.86494753 45.44195344 29.6211094  41.19963792 52.8602289\n",
      " 41.71946    50.55723827 48.41371894 50.21591681 49.87567198 45.00910875\n",
      " 31.61488185 40.78593353 29.53185909 57.12619592 42.03278878 63.18177401\n",
      " 39.55630455 61.17286089 49.31098981 28.20745964 36.85262289 53.79482336\n",
      " 49.64944023 49.42368699 33.76118723 44.4707445  31.61488185 46.09480928\n",
      " 66.77106213 34.52256196 41.51117229 35.19504527 68.46939789 41.09603238\n",
      " 49.64944023 39.55630455 32.35424524 27.68523636 34.61827213 27.85883228\n",
      " 50.67125135 45.55046368 54.14727012 33.66655371 66.2525646  73.41377762\n",
      " 56.88513044 30.6107582  44.5781781  30.97421929 33.38337091 34.14091758\n",
      " 65.86494753 47.08216785 57.85226341 32.44720398 58.09524292 47.41344066\n",
      " 39.55630455 53.79482336 57.6097624  63.68878737 57.36773991 47.52410419\n",
      " 60.05630529 56.2845602  48.07921621 79.50675564 45.87671216 45.11714049\n",
      " 62.80277008 46.09480928 25.38852536 62.67667469 52.39580271 31.52299975\n",
      " 45.00910875 55.68698064 38.1435095  33.47764555 51.01400834 54.61887392\n",
      " 55.56782361 38.04349276 61.17286089 55.21107027 42.45223526 67.68303845\n",
      " 55.8062573  64.58066637 53.79482336 44.79340417 48.41371894 58.46060939\n",
      " 40.6828065  42.45223526 55.68698064 40.37414317 52.39580271 52.8602289\n",
      " 49.31098981 30.06915536 65.09294331 54.85539358 31.70688358 31.89124592\n",
      " 41.30336308 44.4707445  47.96795455 56.0451695  38.44427745 78.80270815\n",
      " 50.55723827 44.5781781  37.04990494 47.08216785 39.35304116 45.33356283\n",
      " 43.72205887 47.74579011 45.76784304 50.44334482 62.04799214 63.81583977\n",
      " 45.00910875 60.42741385 56.0451695  61.7973565  58.94943946 66.38200954\n",
      " 70.45413772 59.68627337 39.76004644 49.42368699 42.97923496 40.5797991\n",
      " 48.5254591  48.6373189  34.90612038 38.34390184 51.01400834 36.36151126\n",
      " 67.68303845 44.5781781  42.87359577 33.28921589 63.68878737 51.01400834\n",
      " 49.9889673  40.99254647 38.64538755 54.38283277 46.42285216 38.94794991\n",
      " 52.39580271 27.85883228 38.94794991 51.81796159 50.10238224 27.59861784\n",
      " 38.24364585 74.91530985 38.34390184 74.77820878 37.54520351 62.04799214\n",
      " 51.12849992 43.08499378 49.5365038  40.37414317 50.21591681 80.63945222\n",
      " 41.8237833  51.47269243 65.4784071  49.08595433 35.58195324 63.05531974\n",
      " 45.11714049 75.18987085 72.33084582 44.5781781  56.64454346 51.93329056\n",
      " 60.55135595 31.1566676  66.12323928 40.37414317 43.72205887 36.9512041\n",
      " 58.46060939 70.45413772 47.52410419 48.97361603 35.58195324 36.75416131\n",
      " 56.404435   51.93329056 50.78538405 42.13747096 29.35371735 74.36762335\n",
      " 49.5365038  34.42697143 34.71410192 44.68573132 51.35784197 31.2480712\n",
      " 52.39580271 56.88513044 65.99403359 67.42187566 48.19059749 54.50079353\n",
      " 44.14916147 48.19059749 48.5254591  51.12849992 30.33941846 27.51211895\n",
      " 44.36343053 47.74579011 55.21107027 25.30513677 54.02966824 79.93061966\n",
      " 56.88513044 65.60713428 30.70144403 42.97923496 60.30359137 57.85226341\n",
      " 68.60087649 65.22131161 37.1487254  64.19771475 47.85681252 43.402988\n",
      " 39.76004644 52.8602289  46.42285216 45.55046368 59.19457226 91.78191871\n",
      " 56.16480504 55.8062573  75.05253054 44.14916147 39.55630455 42.45223526\n",
      " 50.21591681 49.31098981 20.06598459 69.92224198 29.7999689  19.54088161\n",
      " 39.86209683 37.1487254  71.92672025 57.00560336 46.42285216 29.17605411\n",
      " 32.72679798 38.24364585 39.35304116 45.33356283 58.33870094 65.99403359\n",
      " 39.15025628 48.07921621 69.2600639  71.92672025 60.79959905 39.04904328\n",
      " 29.26482592 67.03102866 56.76477714 39.76004644 37.24766549 54.38283277\n",
      " 53.20980462 37.54520351 29.6211094  43.402988   58.21691212 38.24364585\n",
      " 64.70855617 53.91218599 33.38337091 48.6373189  72.19601767 78.24162345\n",
      " 27.77197451 78.38171518 42.55739594 55.56782361 54.97383285 26.73901212\n",
      " 32.63348035 55.68698064 36.26364781 76.84728557 29.4427284  75.05253054\n",
      " 40.99254647 37.54520351 50.89963638 35.29159282 31.79900493 58.58263747\n",
      " 92.99500455 36.75416131 39.25158891 47.41344066 53.20980462 33.57203981\n",
      " 65.86494753 39.25158891 62.17348939 60.67541769 41.19963792 52.74394291\n",
      " 46.97198284 42.24227276 59.68627337 44.36343053 60.42741385 64.32524567\n",
      " 29.35371735 34.90612038 52.51172982 47.52410419 46.86191745 64.07030347\n",
      " 60.79959905 39.86209683 46.53243905 30.88317458 60.30359137 51.58766252\n",
      " 27.08141889 26.48346313 51.47269243 46.86191745 39.65811568 58.94943946\n",
      " 47.1924725  59.07194604 40.27149465 55.68698064 48.07921621 59.56316865\n",
      " 39.96426684 26.22899078 60.05630529 38.84697616 66.64125831 36.0682798\n",
      " 49.19841226 49.9889673  46.75197169 51.12849992 69.2600639  44.79340417\n",
      " 74.09449786 77.68245277 57.36773991 27.68523636 55.8062573  49.31098981\n",
      " 59.19457226 35.09861735 74.91530985 32.91379211 34.04580555 31.79900493\n",
      " 54.26499163 49.19841226 34.90612038 38.64538755 76.98618103 32.63348035\n",
      " 57.2469081  41.19963792 25.80726273 52.8602289  57.85226341 55.92565358\n",
      " 26.48346313 44.36343053 43.402988   53.32656911 60.05630529 33.19518051\n",
      " 51.58766252 49.87567198 55.56782361 45.00910875 42.24227276 39.15025628]\n",
      "20.875169673559295\n",
      "[[1.0000000e+00 5.3600000e+01 2.8729600e+03]\n",
      " [1.0000000e+00 5.1540000e+01 2.6563716e+03]\n",
      " [1.0000000e+00 7.9700000e+01 6.3520900e+03]\n",
      " [1.0000000e+00 5.6680000e+01 3.2126224e+03]\n",
      " [1.0000000e+00 6.8500000e+01 4.6922500e+03]\n",
      " [1.0000000e+00 6.4800000e+01 4.1990400e+03]\n",
      " [1.0000000e+00 5.7400000e+01 3.2947600e+03]\n",
      " [1.0000000e+00 6.3500000e+01 4.0322500e+03]\n",
      " [1.0000000e+00 6.3300000e+01 4.0068900e+03]\n",
      " [1.0000000e+00 6.1400000e+01 3.7699600e+03]\n",
      " [1.0000000e+00 6.9900000e+01 4.8860100e+03]\n",
      " [1.0000000e+00 9.2500000e+01 8.5562500e+03]\n",
      " [1.0000000e+00 8.3100000e+01 6.9056100e+03]\n",
      " [1.0000000e+00 5.1300000e+01 2.6316900e+03]\n",
      " [1.0000000e+00 6.4500000e+01 4.1602500e+03]\n",
      " [1.0000000e+00 6.4200000e+01 4.1216400e+03]\n",
      " [1.0000000e+00 6.4800000e+01 4.1990400e+03]\n",
      " [1.0000000e+00 5.7300000e+01 3.2832900e+03]\n",
      " [1.0000000e+00 5.5500000e+01 3.0802500e+03]\n",
      " [1.0000000e+00 6.7400000e+01 4.5427600e+03]\n",
      " [1.0000000e+00 8.4000000e+01 7.0560000e+03]\n",
      " [1.0000000e+00 6.6400000e+01 4.4089600e+03]\n",
      " [1.0000000e+00 8.4800000e+01 7.1910400e+03]\n",
      " [1.0000000e+00 8.2300000e+01 6.7732900e+03]\n",
      " [1.0000000e+00 5.1400000e+01 2.6419600e+03]\n",
      " [1.0000000e+00 9.1400000e+01 8.3539600e+03]\n",
      " [1.0000000e+00 7.7700000e+01 6.0372900e+03]\n",
      " [1.0000000e+00 5.6400000e+01 3.1809600e+03]\n",
      " [1.0000000e+00 6.6300000e+01 4.3956900e+03]\n",
      " [1.0000000e+00 5.2800000e+01 2.7878400e+03]\n",
      " [1.0000000e+00 8.2900000e+01 6.8724100e+03]\n",
      " [1.0000000e+00 6.8900000e+01 4.7472100e+03]\n",
      " [1.0000000e+00 6.4200000e+01 4.1216400e+03]\n",
      " [1.0000000e+00 6.4100000e+01 4.1088100e+03]\n",
      " [1.0000000e+00 7.6100000e+01 5.7912100e+03]\n",
      " [1.0000000e+00 5.7720000e+01 3.3315984e+03]\n",
      " [1.0000000e+00 4.5220000e+01 2.0448484e+03]\n",
      " [1.0000000e+00 9.0600000e+01 8.2083600e+03]\n",
      " [1.0000000e+00 4.4100000e+01 1.9448100e+03]\n",
      " [1.0000000e+00 6.0900000e+01 3.7088100e+03]\n",
      " [1.0000000e+00 5.0300000e+01 2.5300900e+03]\n",
      " [1.0000000e+00 6.9000000e+01 4.7610000e+03]\n",
      " [1.0000000e+00 7.9800000e+01 6.3680400e+03]\n",
      " [1.0000000e+00 5.9600000e+01 3.5521600e+03]\n",
      " [1.0000000e+00 5.1500000e+01 2.6522500e+03]\n",
      " [1.0000000e+00 7.8700000e+01 6.1936900e+03]\n",
      " [1.0000000e+00 5.6400000e+01 3.1809600e+03]\n",
      " [1.0000000e+00 5.3600000e+01 2.8729600e+03]\n",
      " [1.0000000e+00 5.2100000e+01 2.7144100e+03]\n",
      " [1.0000000e+00 6.6400000e+01 4.4089600e+03]\n",
      " [1.0000000e+00 7.5200000e+01 5.6550400e+03]\n",
      " [1.0000000e+00 5.6000000e+01 3.1360000e+03]\n",
      " [1.0000000e+00 6.0000000e+01 3.6000000e+03]\n",
      " [1.0000000e+00 6.9100000e+01 4.7748100e+03]\n",
      " [1.0000000e+00 6.2000000e+01 3.8440000e+03]\n",
      " [1.0000000e+00 7.3500000e+01 5.4022500e+03]\n",
      " [1.0000000e+00 5.9800000e+01 3.5760400e+03]\n",
      " [1.0000000e+00 7.9100000e+01 6.2568100e+03]\n",
      " [1.0000000e+00 5.2900000e+01 2.7984100e+03]\n",
      " [1.0000000e+00 8.5000000e+01 7.2250000e+03]\n",
      " [1.0000000e+00 8.2600000e+01 6.8227600e+03]\n",
      " [1.0000000e+00 4.8800000e+01 2.3814400e+03]\n",
      " [1.0000000e+00 8.4600000e+01 7.1571600e+03]\n",
      " [1.0000000e+00 8.0400000e+01 6.4641600e+03]\n",
      " [1.0000000e+00 7.3500000e+01 5.4022500e+03]\n",
      " [1.0000000e+00 8.6700000e+01 7.5168900e+03]\n",
      " [1.0000000e+00 5.7800000e+01 3.3408400e+03]\n",
      " [1.0000000e+00 4.9200000e+01 2.4206400e+03]\n",
      " [1.0000000e+00 8.4400000e+01 7.1233600e+03]\n",
      " [1.0000000e+00 6.5500000e+01 4.2902500e+03]\n",
      " [1.0000000e+00 8.9900000e+01 8.0820100e+03]\n",
      " [1.0000000e+00 6.6960000e+01 4.4836416e+03]\n",
      " [1.0000000e+00 8.5100000e+01 7.2420100e+03]\n",
      " [1.0000000e+00 7.9700000e+01 6.3520900e+03]\n",
      " [1.0000000e+00 1.0490000e+02 1.1004010e+04]\n",
      " [1.0000000e+00 5.4280000e+01 2.9463184e+03]\n",
      " [1.0000000e+00 5.6500000e+01 3.1922500e+03]\n",
      " [1.0000000e+00 6.8500000e+01 4.6922500e+03]\n",
      " [1.0000000e+00 7.6920000e+01 5.9166864e+03]\n",
      " [1.0000000e+00 9.7000000e+01 9.4090000e+03]\n",
      " [1.0000000e+00 6.7100000e+01 4.5024100e+03]\n",
      " [1.0000000e+00 7.2500000e+01 5.2562500e+03]\n",
      " [1.0000000e+00 6.3300000e+01 4.0068900e+03]\n",
      " [1.0000000e+00 6.3900000e+01 4.0832100e+03]\n",
      " [1.0000000e+00 7.2800000e+01 5.2998400e+03]\n",
      " [1.0000000e+00 6.5300000e+01 4.2640900e+03]\n",
      " [1.0000000e+00 5.1600000e+01 2.6625600e+03]\n",
      " [1.0000000e+00 6.2400000e+01 3.8937600e+03]\n",
      " [1.0000000e+00 5.2400000e+01 2.7457600e+03]\n",
      " [1.0000000e+00 5.8400000e+01 3.4105600e+03]\n",
      " [1.0000000e+00 4.8200000e+01 2.3232400e+03]\n",
      " [1.0000000e+00 7.5000000e+01 5.6250000e+03]\n",
      " [1.0000000e+00 5.5900000e+01 3.1248100e+03]\n",
      " [1.0000000e+00 5.5300000e+01 3.0580900e+03]\n",
      " [1.0000000e+00 7.8300000e+01 6.1308900e+03]\n",
      " [1.0000000e+00 5.5700000e+01 3.1024900e+03]\n",
      " [1.0000000e+00 8.6900000e+01 7.5516100e+03]\n",
      " [1.0000000e+00 7.0500000e+01 4.9702500e+03]\n",
      " [1.0000000e+00 8.3500000e+01 6.9722500e+03]\n",
      " [1.0000000e+00 5.2500000e+01 2.7562500e+03]]\n",
      "[0.00843566 0.29855564 0.00598134]\n",
      "[33.19518051 31.28466613 61.7973565  36.14636958 48.5254591  44.4707445\n",
      " 36.85262289 43.08499378 42.87359577 40.88918018 50.10238224 78.80270815\n",
      " 66.12323928 31.06538364 44.14916147 43.82865508 44.4707445  36.75416131\n",
      " 35.00230905 47.30289677 67.2914737  46.20403728 68.33803892 65.09294331\n",
      " 31.1566676  77.26433085 59.31731809 35.87339029 46.09480928 32.44720398\n",
      " 65.86494753 48.97361603 43.82865508 43.72205887 57.36773991 37.16850385\n",
      " 25.74006378 76.15460263 24.80731737 40.37414317 30.15912343 49.08595433\n",
      " 61.92261451 39.04904328 31.2480712  60.55135595 35.87339029 33.19518051\n",
      " 31.79900493 46.20403728 56.2845602  35.4850468  39.45461304 49.19841226\n",
      " 41.51117229 54.26499163 39.25158891 61.04832065 32.54028235 68.60087649\n",
      " 65.4784071  28.82216317 68.07567985 62.67667469 54.26499163 70.85431562\n",
      " 37.24766549 29.17605411 67.81379929 45.22529185 75.18987085 46.81792479\n",
      " 68.73247472 61.7973565  97.14569297 33.83698018 35.97077523 48.5254591\n",
      " 58.36307306 85.24679973 46.97198284 53.09315975 42.87359577 43.50922533\n",
      " 53.44345323 45.00910875 31.33959442 41.92822623 32.07608676 37.84381818\n",
      " 28.29491555 56.0451695  35.38826    34.81005133 60.05630529 35.19504527\n",
      " 71.12169901 50.78538405 66.64125831 32.16868663]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# (1) Split data\n",
    "train, val = SplitData(training_datalist, 0.95)\n",
    "\n",
    "# (2) Preprocess data\n",
    "preptrain = PreprocessData(train)\n",
    "prepval = PreprocessData(val)\n",
    "\n",
    "# (3) Train regression model\n",
    "grad = Regression(preptrain)\n",
    "print(grad)\n",
    "\n",
    "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
    "pred = MakePrediction(grad, prepval)\n",
    "gt = prepval[:, 1]\n",
    "mape = np.mean(np.abs((gt - pred)/gt))*100\n",
    "print(mape)\n",
    "\n",
    "# (5) Make prediction of testing dataset and store the values in output_datalist\n",
    "output_datalist = MakePrediction(grad, testing_datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW3NrFmGEEiG"
   },
   "source": [
    "### *Write the Output File*\n",
    "\n",
    "Write the prediction to output csv and upload the file to Kaggle\n",
    "> Format: 'Id', 'gripForce'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Mo7rdhx0EFLn"
   },
   "outputs": [],
   "source": [
    "# Assume that output_datalist is a list (or 1d array) with length = 100\n",
    "\n",
    "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['Id', 'gripForce'])\n",
    "  for i in range(len(output_datalist)):\n",
    "    writer.writerow([i,output_datalist[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1O2l8d2E3he"
   },
   "source": [
    "# 2. Advanced Part (45%)\n",
    "In the second part, you need to implement regression differently from the basic part to improve your grip force predictions. You must use more than two features.\n",
    "\n",
    "You can choose either matrix inversion or gradient descent for this part\n",
    "\n",
    "We have provided `lab1_advanced_training.csv` for your training\n",
    "\n",
    "> Notice: Be cautious of the \"gender\" attribute, as it is represented by \"F\"/\"M\" rather than a numerical value.\n",
    "\n",
    "Please save the prediction result in a CSV file and submit it to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Us1kieIvcucL"
   },
   "outputs": [],
   "source": [
    "training_dataroot = 'lab1_advanced_training.csv' # Training data file file named as 'lab1_advanced_training.csv'\n",
    "testing_dataroot = 'lab1_advanced_testing.csv'   # Testing data file named as 'lab1_advanced_testing.csv'\n",
    "output_dataroot = 'lab1_advanced.csv' # Output file will be named as 'lab1_advanced.csv'\n",
    "\n",
    "training_datalist =  [] # Training datalist, saved as numpy array\n",
    "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
    "\n",
    "output_datalist =  [] # Your prediction, should be a list with 3000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zAmzFZM-dCkG"
   },
   "outputs": [],
   "source": [
    "# Read input csv to datalist\n",
    "with open(training_dataroot, newline='') as csvfile:\n",
    "  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
    "\n",
    "with open(testing_dataroot, newline='') as csvfile:\n",
    "  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(data, split_ratio):\n",
    "    \"\"\"\n",
    "    Splits the given dataset into training and validation sets based on the specified split ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to be split. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "    - split_ratio (float): The ratio of the data to be used for training. For example, a value of 0.8 means 80% of the data will be used for training and the remaining 20% for validation.\n",
    "\n",
    "    Returns:\n",
    "    - training_data (numpy.ndarray): The portion of the dataset used for training.\n",
    "    - validation_data (numpy.ndarray): The portion of the dataset used for validation.\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx]\n",
    "    mid = int(data.shape[0] * split_ratio)\n",
    "    training_data = data[:mid]\n",
    "    validation_data = data[mid:]\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return training_data, validation_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessData(data):\n",
    "    \"\"\"\n",
    "    Preprocess the given dataset and return the result.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to preprocess. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "\n",
    "    Returns:\n",
    "    - preprocessedData (numpy.ndarray): Preprocessed data.\n",
    "    \"\"\"\n",
    "    for i in range(data.shape[0]):\n",
    "        if data[i][1] == 'F':\n",
    "            data[i][1] = 0\n",
    "        elif data[i][1] == 'M':\n",
    "            data[i][1] = 1\n",
    "    data = np.array(data, dtype = float)\n",
    "    data = data[data[:, 7] > 0]\n",
    "    data = data[data[:, 7] < 150]\n",
    "    \n",
    "    preprocessedData = data\n",
    "    for i in range(8):\n",
    "        if i != 1:\n",
    "            q1 = np.nanpercentile(data[:,i], 25)\n",
    "            q3 = np.nanpercentile(data[:,i], 75)\n",
    "\n",
    "            iqr = q3-q1\n",
    "            lowerbound = q1 - (1.5 * iqr)\n",
    "            upperbound = q3 + (1.5 * iqr)\n",
    "\n",
    "            detect = (preprocessedData[:, i] >= lowerbound) & (preprocessedData[:,i] <= upperbound)\n",
    "            preprocessedData = preprocessedData[detect]\n",
    "    \n",
    "    avg = np.nanmean(preprocessedData, axis = 0)\n",
    "    uniqueval, cnt = np.unique(preprocessedData[:, 1][~np.isnan(preprocessedData[:, 1])], return_counts = True)\n",
    "    avg[1] = uniqueval[np.argmax(cnt)]\n",
    "    for i in range(preprocessedData.shape[1]):\n",
    "        preprocessedData[np.isnan(preprocessedData[:, i]), i] = avg[i]\n",
    "        #TRY REMOVE OUTLIER FIRST BEFORE REPLACING NAN WITH AVG\n",
    "    \n",
    "\n",
    "    # TODO\n",
    "    low = np.min(preprocessedData, axis = 0)\n",
    "    high = np.max(preprocessedData, axis = 0)\n",
    "    preprocessedData = (preprocessedData - low)/(high-low)\n",
    "\n",
    "    return preprocessedData, low, high\n",
    "\n",
    "def PreprocessDataTest(data, low, high):\n",
    "    for i in range(data.shape[0]):\n",
    "        if data[i][1] == 'F':\n",
    "            data[i][1] = 0\n",
    "        elif data[i][1] == 'M':\n",
    "            data[i][1] = 1\n",
    "    if data.shape[1] == 8:\n",
    "        data = data[data[:, 7] > 0]\n",
    "        data = data[data[:, 7] < 150]\n",
    "    data = np.array(data, dtype = float)\n",
    "    avg = []\n",
    "    for i in range(data.shape[1]):\n",
    "        if i != 1:\n",
    "            feat = data[:, i]\n",
    "            q1 = np.nanpercentile(feat, 25)\n",
    "            q3 = np.nanpercentile(feat, 75)\n",
    "            \n",
    "            iqr = q3 - q1\n",
    "            lowerbound = q1 - (1.5 * iqr)\n",
    "            upperbound = q3 + (1.5 * iqr)\n",
    "            print(lowerbound, upperbound)\n",
    "            print(feat[(feat >=lowerbound) & (feat <=upperbound)])\n",
    "            avg.append(np.mean(feat[(feat >= lowerbound) & (feat <= upperbound)]))\n",
    "            \n",
    "            data[:, i] = np.where((feat < lowerbound) | (feat > upperbound), avg[i], feat)\n",
    "        else:\n",
    "            avg.append(0)\n",
    "    \n",
    "    uniqueval, cnt = np.unique(data[:, 1][~np.isnan(data[:, 1])], return_counts = True)\n",
    "    avg[1] = uniqueval[np.argmax(cnt)]\n",
    "    print(avg)\n",
    "    print(data)\n",
    "    for i in range(data.shape[1]):\n",
    "        data[np.isnan(data[:, i]), i] = avg[i]\n",
    "    preprocessedData = (data-low)/(high-low)\n",
    "    \n",
    "    return preprocessedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression(dataset):\n",
    "    \"\"\"\n",
    "    Performs regression on the given dataset and return the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n",
    "\n",
    "    Returns:\n",
    "    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x + w[2] * x^2 + ...\n",
    "    \"\"\"\n",
    "    \n",
    "    X = dataset[:, :7]\n",
    "    y = dataset[:, 7]\n",
    "    \n",
    "    # TODO: Decide on the degree of the polynomial\n",
    "    degree = 3  # For example, quadratic regression\n",
    "\n",
    "    # Add polynomial features to X\n",
    "    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, X**d)) # Add x^d terms to feature matrix\n",
    "    #for i in range(X.shape[1]-1):\n",
    "     #   for j in range(i+1, X.shape[1]):\n",
    "      #      X_poly = np.hstack((X_poly, np.array([X[:, i] * X[:, j]]).T))\n",
    "        \n",
    "    # Initialize coefficients (weights) to zero\n",
    "    num_dimensions = X_poly.shape[1]  # Number of features (including intercept and polynomial terms)\n",
    "    w = np.zeros(num_dimensions)\n",
    "\n",
    "    # TODO: Set hyperparameters\n",
    "    num_iteration = 10000\n",
    "    learning_rate = 0.01\n",
    "\n",
    "\n",
    "    # Gradient Descent\n",
    "    m = len(y)  # Number of data points\n",
    "    lambdaval = 0.1\n",
    "    for iteration in range(num_iteration):\n",
    "        # TODO: Prediction using current weights and compute error\n",
    "        y_predict = np.dot(X_poly, w)\n",
    "        #print(y_predict)\n",
    "        err = y_predict - y\n",
    "        #print(err)\n",
    "\n",
    "        # TODO: Compute gradient\n",
    "        gradient = 2/m * np.dot(X_poly.T, err) + 2 *lambdaval*w\n",
    "        #print(gradient)\n",
    "\n",
    "        # TODO: Update the weights\n",
    "        w -= (learning_rate * gradient)\n",
    "        #print(w)\n",
    "        \n",
    "        # TODO: Optionally, print the cost every 100 iterations\n",
    "        if iteration % 100 == 0:\n",
    "            cost = np.sum(err**2) / m + lambdaval * np.sum(w**2)\n",
    "            print(f\"Iteration {iteration}, Cost: {cost}\")\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MakePrediction(w, test_dataset):\n",
    "    \"\"\"\n",
    "    Predicts the output for a given test dataset using a regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n",
    "                               a coefficient for the respective power of the independent variable.\n",
    "    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n",
    "                                          for which predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    X = test_dataset[:, :7]\n",
    "    # TODO\n",
    "    degree = 3\n",
    "    X_poly = np.ones((X.shape[0], 1))\n",
    "    for d in range(1, degree+1):\n",
    "        X_poly = np.hstack((X_poly, X**d))\n",
    "    #for i in range(X.shape[1]-1):\n",
    "     #   for j in range(i+1, X.shape[1]):\n",
    "      #      X_poly = np.hstack((X_poly, np.array([X[:, i] * X[:, j]]).T))\n",
    "    \n",
    "    prediction = np.dot(X_poly, w)\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "NSMzOXFAo2P0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.5 82.5\n",
      "[43. 21. 22. ... 36. 46. 61.]\n",
      "143.85000000000002 193.04999999999995\n",
      "[170.  177.2 184.6 ... 156.2 162.3 158.4]\n",
      "30.45 102.85000000000001\n",
      "[73.8 64.2 90.6 ... 63.5 61.2 54.7]\n",
      "3.0 43.0\n",
      "[22.7 15.3 23.3 ... 34.3 21.7 27.8]\n",
      "48.5 108.5\n",
      "[86. 78. 64. ... 83. 78. 73.]\n",
      "88.5 172.5\n",
      "[126. 139. 125. ... 108. 123. 145.]\n",
      "0.30000000000000426 87.5\n",
      "[50.9 43.1 57.2 ... 32.2 37.6 27.7]\n",
      "[36.97740846533368, 1.0, 168.49683937823835, 67.06708160442601, 23.048717430171962, 78.90252341311134, 130.4124513618677, 44.261895959862684]\n",
      "[[ 43.    1.  170.  ...  86.  126.   50.9]\n",
      " [ 21.    1.  177.2 ...  78.  139.   43.1]\n",
      " [ 22.    1.  184.6 ...  64.  125.   57.2]\n",
      " ...\n",
      " [ 36.    0.  156.2 ...  83.  108.   32.2]\n",
      " [ 46.    0.  162.3 ...  78.  123.   37.6]\n",
      " [ 61.    0.  158.4 ...  73.  145.   27.7]]\n",
      "-8.0 80.0\n",
      "[39. 27. 36. ... 21. 27. 55.]\n",
      "145.45000000000002 192.24999999999997\n",
      "[180.1 174.3 168.6 ... 168.4 164.1 169.6]\n",
      "32.5625 101.4625\n",
      "[73.8 79.6 59.1 ... 66.7 52.3 67.3]\n",
      "3.599999999999998 42.0\n",
      "[28.1 20.5 31.4 ... 22.9 26.6 21.6]\n",
      "48.5 108.5\n",
      "[80. 60. 74. ... 70. 70. 71.]\n",
      "90.0 170.0\n",
      "[133. 136. 106. ... 119. 120. 127.]\n",
      "[36.545, 1.0, 168.7641975308642, 67.432330877428, 22.919637257794168, 78.94842598794374, 129.95554812834226]\n",
      "[[ 39.    1.  180.1 ...  28.1  80.  133. ]\n",
      " [ 27.    1.  174.3 ...  20.5  60.  136. ]\n",
      " [ 36.    0.  168.6 ...  31.4  74.  106. ]\n",
      " ...\n",
      " [ 21.    1.  168.4 ...  22.9  70.  119. ]\n",
      " [ 27.    0.  164.1 ...  26.6  70.  120. ]\n",
      " [ 55.    1.  169.6 ...  21.6  71.  127. ]]\n",
      "Iteration 0, Cost: 0.28195503630388524\n",
      "Iteration 100, Cost: 0.016140894637251144\n",
      "Iteration 200, Cost: 0.015494811494860455\n",
      "Iteration 300, Cost: 0.015259970721660663\n",
      "Iteration 400, Cost: 0.015163962039609326\n",
      "Iteration 500, Cost: 0.015120190664190895\n",
      "Iteration 600, Cost: 0.015098449110325284\n",
      "Iteration 700, Cost: 0.015086984253367824\n",
      "Iteration 800, Cost: 0.015080693895881073\n",
      "Iteration 900, Cost: 0.015077149137652672\n",
      "Iteration 1000, Cost: 0.015075112844292558\n",
      "Iteration 1100, Cost: 0.01507392533337764\n",
      "Iteration 1200, Cost: 0.015073223867821383\n",
      "Iteration 1300, Cost: 0.015072804664339363\n",
      "Iteration 1400, Cost: 0.01507255137478284\n",
      "Iteration 1500, Cost: 0.015072396690253597\n",
      "Iteration 1600, Cost: 0.015072301221378637\n",
      "Iteration 1700, Cost: 0.015072241673405838\n",
      "Iteration 1800, Cost: 0.015072204132152696\n",
      "Iteration 1900, Cost: 0.015072180206434891\n",
      "Iteration 2000, Cost: 0.015072164788137431\n",
      "Iteration 2100, Cost: 0.015072154738790238\n",
      "Iteration 2200, Cost: 0.015072148112241378\n",
      "Iteration 2300, Cost: 0.015072143690489433\n",
      "Iteration 2400, Cost: 0.015072140704103936\n",
      "Iteration 2500, Cost: 0.015072138662396997\n",
      "Iteration 2600, Cost: 0.015072137249400902\n",
      "Iteration 2700, Cost: 0.015072136259627139\n",
      "Iteration 2800, Cost: 0.015072135558067894\n",
      "Iteration 2900, Cost: 0.01507213505508764\n",
      "Iteration 3000, Cost: 0.015072134690531718\n",
      "Iteration 3100, Cost: 0.015072134423584585\n",
      "Iteration 3200, Cost: 0.015072134226242246\n",
      "Iteration 3300, Cost: 0.015072134079074357\n",
      "Iteration 3400, Cost: 0.015072133968448045\n",
      "Iteration 3500, Cost: 0.015072133884692486\n",
      "Iteration 3600, Cost: 0.015072133820873787\n",
      "Iteration 3700, Cost: 0.015072133771969191\n",
      "Iteration 3800, Cost: 0.015072133734304815\n",
      "Iteration 3900, Cost: 0.01507213370516883\n",
      "Iteration 4000, Cost: 0.015072133682542625\n",
      "Iteration 4100, Cost: 0.015072133664911953\n",
      "Iteration 4200, Cost: 0.015072133651132916\n",
      "Iteration 4300, Cost: 0.015072133640335949\n",
      "Iteration 4400, Cost: 0.01507213363185627\n",
      "Iteration 4500, Cost: 0.015072133625183096\n",
      "Iteration 4600, Cost: 0.015072133619922224\n",
      "Iteration 4700, Cost: 0.015072133615768223\n",
      "Iteration 4800, Cost: 0.015072133612483623\n",
      "Iteration 4900, Cost: 0.015072133609883222\n",
      "Iteration 5000, Cost: 0.015072133607822199\n",
      "Iteration 5100, Cost: 0.01507213360618704\n",
      "Iteration 5200, Cost: 0.015072133604888571\n",
      "Iteration 5300, Cost: 0.015072133603856617\n",
      "Iteration 5400, Cost: 0.015072133603035862\n",
      "Iteration 5500, Cost: 0.015072133602382635\n",
      "Iteration 5600, Cost: 0.015072133601862419\n",
      "Iteration 5700, Cost: 0.01507213360144789\n",
      "Iteration 5800, Cost: 0.015072133601117397\n",
      "Iteration 5900, Cost: 0.015072133600853778\n",
      "Iteration 6000, Cost: 0.015072133600643405\n",
      "Iteration 6100, Cost: 0.015072133600475452\n",
      "Iteration 6200, Cost: 0.01507213360034131\n",
      "Iteration 6300, Cost: 0.015072133600234133\n",
      "Iteration 6400, Cost: 0.015072133600148469\n",
      "Iteration 6500, Cost: 0.015072133600079984\n",
      "Iteration 6600, Cost: 0.015072133600025205\n",
      "Iteration 6700, Cost: 0.015072133599981385\n",
      "Iteration 6800, Cost: 0.01507213359994632\n",
      "Iteration 6900, Cost: 0.015072133599918255\n",
      "Iteration 7000, Cost: 0.015072133599895782\n",
      "Iteration 7100, Cost: 0.015072133599877788\n",
      "Iteration 7200, Cost: 0.015072133599863372\n",
      "Iteration 7300, Cost: 0.015072133599851822\n",
      "Iteration 7400, Cost: 0.015072133599842566\n",
      "Iteration 7500, Cost: 0.015072133599835152\n",
      "Iteration 7600, Cost: 0.015072133599829202\n",
      "Iteration 7700, Cost: 0.015072133599824435\n",
      "Iteration 7800, Cost: 0.015072133599820613\n",
      "Iteration 7900, Cost: 0.015072133599817544\n",
      "Iteration 8000, Cost: 0.015072133599815086\n",
      "Iteration 8100, Cost: 0.01507213359981311\n",
      "Iteration 8200, Cost: 0.015072133599811527\n",
      "Iteration 8300, Cost: 0.015072133599810258\n",
      "Iteration 8400, Cost: 0.015072133599809237\n",
      "Iteration 8500, Cost: 0.015072133599808416\n",
      "Iteration 8600, Cost: 0.015072133599807759\n",
      "Iteration 8700, Cost: 0.01507213359980723\n",
      "Iteration 8800, Cost: 0.015072133599806808\n",
      "Iteration 8900, Cost: 0.015072133599806468\n",
      "Iteration 9000, Cost: 0.015072133599806192\n",
      "Iteration 9100, Cost: 0.015072133599805972\n",
      "Iteration 9200, Cost: 0.015072133599805797\n",
      "Iteration 9300, Cost: 0.015072133599805655\n",
      "Iteration 9400, Cost: 0.01507213359980554\n",
      "Iteration 9500, Cost: 0.015072133599805448\n",
      "Iteration 9600, Cost: 0.015072133599805373\n",
      "Iteration 9700, Cost: 0.015072133599805315\n",
      "Iteration 9800, Cost: 0.015072133599805266\n",
      "Iteration 9900, Cost: 0.01507213359980523\n",
      "GRAD: [ 0.16984803  0.01557255  0.06490648  0.08663597  0.07602611  0.04301952\n",
      "  0.05160349  0.05443894 -0.01166319  0.06490648  0.04842549  0.03727034\n",
      "  0.00551628  0.01396999  0.01565714 -0.01771873  0.06490648  0.02897182\n",
      "  0.01930842 -0.0059685   0.00108037  0.00127008]\n",
      "[0.58053527 0.58289764 0.65350337 ... 0.32105847 0.325685   0.31606786]\n",
      "[50.9 43.1 57.2 ... 32.2 37.6 27.7]\n",
      "[50.16194589 50.35589637 56.15262675 ... 28.85890077 29.23873847\n",
      " 28.44917165]\n",
      "[0.73805411 7.25589637 1.04737325 ... 3.34109923 8.36126153 0.74917165]\n",
      "MAPE: 14.15310681359983\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# (1) Split data\n",
    "train, val = SplitData(training_datalist, 0.6)\n",
    "\n",
    "# (2) Preprocess data\n",
    "preptrain, a, b = PreprocessData(train)\n",
    "prepval = PreprocessDataTest(val, a, b)\n",
    "preptest = PreprocessDataTest(testing_datalist, a[:7], b[:7])\n",
    "\n",
    "# (3) Train regression model\n",
    "grad = Regression(preptrain)\n",
    "print(f'GRAD:', grad)\n",
    "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
    "pred = MakePrediction(grad, prepval)\n",
    "print(pred)\n",
    "gt = prepval[:, 7]\n",
    "pred = pred * (b[7] - a[7]) + a[7]\n",
    "gt = gt * (b[7] - a[7]) + a[7]\n",
    "print(gt)\n",
    "print(pred)\n",
    "print(np.abs((gt-pred)))\n",
    "mape = np.mean(np.abs((gt - pred)/gt))*100\n",
    "print(f'MAPE:', mape)\n",
    "\n",
    "# (5) Make prediction of testing dataset and store the values in output_datalist\n",
    "output_datalist = MakePrediction(grad, preptest)\n",
    "output_datalist = output_datalist * (b[7] - a[7]) + a[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that output_datalist is a list (or 1d array) with length = 100\n",
    "\n",
    "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['Id', 'gripForce'])\n",
    "  for i in range(len(output_datalist)):\n",
    "    writer.writerow([i,output_datalist[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVz38ASe-gGV"
   },
   "source": [
    "# Save the Code File\n",
    "Please save your code and submit it as an ipynb file! (**Lab1.ipynb**)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
